{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5065d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrayReconstructionDataset(Dataset):\n",
    "    def __init__(self, gray_dir, bw_dir, transform_gray=None, transform_bw=None):\n",
    "        self.gray_dir = Path(gray_dir)\n",
    "        self.bw_dir = Path(bw_dir)\n",
    "        self.transform_gray = transform_gray\n",
    "        self.transform_bw = transform_bw\n",
    "\n",
    "        gray_files = list(self.gray_dir.glob(\"*.*\"))\n",
    "        bw_files = list(self.bw_dir.glob(\"*.*\"))\n",
    "        \n",
    "        bw_dict = {}\n",
    "        for f in bw_files:\n",
    "            name = f.stem.lower()\n",
    "            if name.endswith(\"_czb\"):\n",
    "                name = name[:-4]  # usuń '_czb'\n",
    "            bw_dict[name] = f\n",
    "        \n",
    "        self.paired_files = []\n",
    "        for gray_path in gray_files:\n",
    "            stem = gray_path.stem.lower()\n",
    "            if stem in bw_dict:\n",
    "                self.paired_files.append((bw_dict[stem], gray_path))\n",
    "            else:\n",
    "                print(f\"[!] Brak czb dla: {gray_path.stem}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paired_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        bw_path, gray_path = self.paired_files[idx]\n",
    "\n",
    "        bw_img = Image.open(bw_path).convert(\"L\")\n",
    "        gray_img = Image.open(gray_path).convert(\"L\")\n",
    "\n",
    "        if self.transform_bw:\n",
    "            bw_img = self.transform_bw(bw_img)\n",
    "        if self.transform_gray:\n",
    "            gray_img = self.transform_gray(gray_img)\n",
    "\n",
    "        return bw_img, gray_img\n",
    "\n",
    "# UNet model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "\n",
    "        self.enc1 = conv_block(1, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "        self.final = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        b = self.bottleneck(self.pool3(e3))\n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        out = self.final(d1)\n",
    "        return self.activation(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8c61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba sparowanych plików w dataset: 21625\n"
     ]
    }
   ],
   "source": [
    "# Transformaty dla czarno-białych i kolorowych obrazów\n",
    "transform_bw = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_gray = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# === Ścieżki ===\n",
    "gray_dir = \"\"      #folder z kwiatami szarymi\n",
    "bw_dir = \"\"         #folder z szarymi czarnymi\n",
    "\n",
    "# Dataset\n",
    "dataset = GrayReconstructionDataset(gray_dir, bw_dir, transform_gray, transform_bw)\n",
    "\n",
    "print(f\"Liczba sparowanych plików w dataset: {len(dataset)}\")\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# liczba sparowanych plików powinna wynosić 21625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epo 1/5] Epoch [1/4], Loss: 0.0481\n",
      "[Epo 1/5] Epoch [2/4], Loss: 0.0383\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epo = 5\n",
    "num_epochs = 4\n",
    "\n",
    "# Pełny dataset (utwórz raz)\n",
    "full_dataset = GrayReconstructionDataset(\n",
    "    \n",
    "    # === Ścieżki ===\n",
    "    gray_dir = \"\"      #folder z kwiatami szarymi\n",
    "    bw_dir = \"\"         #folder z szarymi czarnymi\n",
    "    \n",
    "    transform_gray=transform_gray,\n",
    "    transform_bw=transform_bw\n",
    ")\n",
    "\n",
    "for epos in range(epo):\n",
    "    # === LOSUJEMY 500 losowych obrazów na aktualny cykl treningowy ===\n",
    "    subset_indices = random.sample(range(len(full_dataset)), min(500, len(full_dataset)))\n",
    "    subset = Subset(full_dataset, subset_indices)\n",
    "    dataloader = DataLoader(subset, batch_size=8, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for bw, color in dataloader:\n",
    "            bw, color = bw.to(device), color.to(device)\n",
    "\n",
    "            output = model(bw)\n",
    "            loss = criterion(output, color)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"[Epo {epos+1}/{epo}] Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # === Zapis modelu po każdej pełnej epoce (czyli po num_epochs iteracjach) ===\n",
    "    save_path = f\"C:\\\\zdjęcia na chwile\\\\kwiaty_same_model_i_data\\\\model_czb_to_szary_kwoaty_{epos+1}_{loss.item():.4f}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Zapisano model: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28342263",
   "metadata": {},
   "source": [
    "dodatkowy trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6932e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "\n",
    "model_path=\"\"    #model który chcesz dotrenować\n",
    "\n",
    "\n",
    "# Załaduj zapisane wcześniej wagi modelu\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epo = 5\n",
    "num_epochs = 4\n",
    "for i in range(epo):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for bw, color in dataloader:\n",
    "            bw, color = bw.to(device), color.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(bw)\n",
    "            loss = criterion(output, color)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * bw.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "    zapis_path=f\"\"    #gdzie zapisać ten model (najlepiej daj tą samą nazwę ale z jakimś dopiskiem np. \"_tren_{epo+1}_{epoch_loss:.4f}.pth\")\n",
    "    torch.save(model.state_dict(), zapis_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3847b9",
   "metadata": {},
   "source": [
    "WCZYTANIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61946f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapisano kolorowy obraz o oryginalnej rozdzielczości: C:\\zdjęcia na chwile\\kwiaty_same_model_i_data\\wynik_kolor_fullres.png\n"
     ]
    }
   ],
   "source": [
    "# === Ścieżki ===\n",
    "input_path = \"\"\n",
    "output_path = \"\"\n",
    "model_path =\"\"\n",
    "\n",
    "# === Wczytaj model ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === Wczytaj obraz czarno-biały ===\n",
    "if not os.path.exists(input_path):\n",
    "    print(f\"Plik {input_path} nie istnieje.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "img = Image.open(input_path).convert(\"L\")\n",
    "original_size = img.size  # zapamiętaj oryginalny rozmiar\n",
    "\n",
    "# === Skaluj do 128x128 dla modelu ===\n",
    "transform_bw = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "bw_tensor = transform_bw(img).unsqueeze(0).to(device)\n",
    "\n",
    "# === Przewidź kolory ===\n",
    "with torch.no_grad():\n",
    "    output = model(bw_tensor)\n",
    "\n",
    "# === Przeskaluj wynik do oryginalnego rozmiaru ===\n",
    "output_resized = transforms.functional.resize(output.squeeze(0), original_size)\n",
    "\n",
    "# === Zapisz wynik ===\n",
    "save_image(output_resized, output_path)\n",
    "print(f\"Zapisano kolorowy obraz o oryginalnej rozdzielczości: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c320c2",
   "metadata": {},
   "source": [
    "teraz ten model musi przerobić wszystkie zdjęcia z folderu z samymi czarno-białymi, do innego folderu, potem te obrazki będą potrzebne do treningu                                            \n",
    "now this model needs to process all the photos from the folder with only black and white ones to another folder, then these images will be needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(input_path, output_path, model, device):\n",
    "    img = Image.open(input_path).convert(\"L\")\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
    "    input_tensor, pad_w, pad_h = pad_tensor_to_multiple(input_tensor, multiple=16)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "\n",
    "    if pad_h > 0:\n",
    "        output = output[:, :, :-pad_h, :]\n",
    "    if pad_w > 0:\n",
    "        output = output[:, :, :, :-pad_w]\n",
    "\n",
    "    output = output * 0.5 + 0.5\n",
    "    output = torch.clamp(output, 0, 1)\n",
    "\n",
    "    output_img = transforms.ToPILImage()(output.squeeze(0).cpu())\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    output_img.save(output_path, format=\"JPEG\")\n",
    "    print(f\"Obraz zapisany do: {output_path}\")\n",
    "\n",
    "def process_folder(input_dir, output_dir, model_path, device):\n",
    "    model = UNet().to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    supported_ext = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
    "\n",
    "    for root, _, files in os.walk(input_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(supported_ext):\n",
    "                input_path = os.path.join(root, file)\n",
    "                rel_path = os.path.relpath(input_path, input_dir)\n",
    "                name, _ = os.path.splitext(rel_path)\n",
    "                # Dodaj \"_szaremodel\" i rozszerzenie .jpg\n",
    "                output_rel_path = f\"{name}_szaremodel.jpg\"\n",
    "                output_path = os.path.join(output_dir, output_rel_path)\n",
    "                process_image(input_path, output_path, model, device)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    input_folder = r\"\"   #folder z czarnymi\n",
    "    output_folder = r\"\"   #folder z wynikami \n",
    "    model_path = r\"\"     #najlepszy model\n",
    "\n",
    "    process_folder(input_folder, output_folder, model_path, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
