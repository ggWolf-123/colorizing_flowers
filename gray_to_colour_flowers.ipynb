{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5065d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, gray_dir, color_dir, transform_gray=None, transform_color=None):\n",
    "        self.gray_dir = Path(gray_dir)\n",
    "        self.color_dir = Path(color_dir)\n",
    "        self.transform_gray = transform_gray\n",
    "        self.transform_color = transform_color\n",
    "\n",
    "        gray_files = list(self.gray_dir.glob(\"*.*\"))\n",
    "        color_files = list(self.color_dir.glob(\"*.*\"))\n",
    "        \n",
    "        # Mapowanie kolorowych obrazów według nazwy\n",
    "        color_dict = {}\n",
    "        for f in color_files:\n",
    "            name = f.stem.lower()\n",
    "            if name.endswith(\"_color\"):\n",
    "                name = name[:-6]  # usuń '_color'\n",
    "            color_dict[name] = f\n",
    "\n",
    "        self.paired_files = []\n",
    "        for gray_path in gray_files:\n",
    "            stem = gray_path.stem.lower()\n",
    "            if stem.endswith(\"_gray\") or stem.endswith(\"_czb\"):\n",
    "                stem = stem.rsplit(\"_\", 1)[0]\n",
    "\n",
    "            if stem in color_dict:\n",
    "                self.paired_files.append((gray_path, color_dict[stem]))\n",
    "            else:\n",
    "                print(f\"[!] Brak koloru dla: {gray_path.name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paired_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gray_path, color_path = self.paired_files[idx]\n",
    "\n",
    "        gray_img = Image.open(gray_path).convert(\"L\")   # 1-kanałowe\n",
    "        color_img = Image.open(color_path).convert(\"RGB\")  # 3-kanałowe\n",
    "\n",
    "        if self.transform_gray:\n",
    "            gray_img = self.transform_gray(gray_img)\n",
    "        if self.transform_color:\n",
    "            color_img = self.transform_color(color_img)\n",
    "\n",
    "        return gray_img, color_img\n",
    "\n",
    "\n",
    "\n",
    "# UNet model\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        def conv_block(in_c, out_c):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_c),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "\n",
    "        self.enc1 = conv_block(1, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "        self.final = nn.Conv2d(64, 1, kernel_size=1)\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        b = self.bottleneck(self.pool3(e3))\n",
    "        d3 = self.dec3(torch.cat([self.up3(b), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "        out = self.final(d1)\n",
    "        return self.activation(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8c61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba sparowanych plików w dataset: 21625\n"
     ]
    }
   ],
   "source": [
    "# Transformaty dla szarych i kolorowych obrazów\n",
    "transform_gray = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_color = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# === Ścieżki ===\n",
    "color_dir = \"\"      #folder z kwiatami kolorowymi\n",
    "bw_dir = \"\"         #folder z szarymi kwiatami\n",
    "\n",
    "# Dataset\n",
    "dataset = ColorizationDataset(color_dir, bw_dir, transform_gray, transform_color)\n",
    "\n",
    "print(f\"Liczba sparowanych plików w dataset: {len(dataset)}\")\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# liczba sparowanych plików powinna wynosić 21625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epo 1/5] Epoch [1/4], Loss: 0.0481\n",
      "[Epo 1/5] Epoch [2/4], Loss: 0.0383\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epo = 5\n",
    "num_epochs = 4\n",
    "\n",
    "# Pełny dataset (utwórz raz)\n",
    "full_dataset = ColorizationDataset(\n",
    "    \n",
    "    # === Ścieżki ===\n",
    "    color_dir=\"\",         #folder z kwiatami kolorowymi  \n",
    "    bw_dir=\"\",            #folder z czarno-białymi  \n",
    "    \n",
    "    transform_color=transform_color,\n",
    "    transform_gray=transform_gray\n",
    ")\n",
    "\n",
    "for epos in range(epo):\n",
    "    # === LOSUJEMY 500 losowych obrazów na aktualny cykl treningowy ===\n",
    "    subset_indices = random.sample(range(len(full_dataset)), min(500, len(full_dataset)))\n",
    "    subset = Subset(full_dataset, subset_indices)\n",
    "    dataloader = DataLoader(subset, batch_size=8, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for bw, color in dataloader:\n",
    "            bw, color = bw.to(device), color.to(device)\n",
    "\n",
    "            output = model(bw)\n",
    "            loss = criterion(output, color)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"[Epo {epos+1}/{epo}] Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # === Zapis modelu po każdej pełnej epoce (czyli po num_epochs iteracjach) ===\n",
    "    save_path = f\"C:\\\\zdjęcia na chwile\\\\kwiaty_same_model_i_data\\\\model_gray_to_color_flowers_{epos+1}_{loss.item():.4f}.pth\"\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Zapisano model: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28342263",
   "metadata": {},
   "source": [
    "dodatkowy trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6932e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "\n",
    "model_path=\"\"    #model który chcesz dotrenować\n",
    "\n",
    "\n",
    "# Załaduj zapisane wcześniej wagi modelu\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epo = 5\n",
    "num_epochs = 4\n",
    "for i in range(epo):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for bw, color in dataloader:\n",
    "            bw, color = bw.to(device), color.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(bw)\n",
    "            loss = criterion(output, color)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * bw.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "    zapis_path=f\"\"    #gdzie zapisać ten model (najlepiej daj tą samą nazwę ale z jakimś dopiskiem np. \"_tren_{epo+1}_{epoch_loss:.4f}.pth\")\n",
    "    torch.save(model.state_dict(), zapis_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3847b9",
   "metadata": {},
   "source": [
    "WCZYTANIE-one photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61946f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapisano kolorowy obraz o oryginalnej rozdzielczości: C:\\zdjęcia na chwile\\kwiaty_same_model_i_data\\wynik_kolor_fullres.png\n"
     ]
    }
   ],
   "source": [
    "# === Ścieżki ===\n",
    "input_path = \"\"\n",
    "output_path = \"\"\n",
    "model_path =\"\"\n",
    "\n",
    "# === Wczytaj model ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === Wczytaj obraz czarno-biały ===\n",
    "if not os.path.exists(input_path):\n",
    "    print(f\"Plik {input_path} nie istnieje.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "img = Image.open(input_path).convert(\"L\")\n",
    "original_size = img.size  # zapamiętaj oryginalny rozmiar\n",
    "\n",
    "# === Skaluj do 128x128 dla modelu ===\n",
    "transform_bw = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "bw_tensor = transform_bw(img).unsqueeze(0).to(device)\n",
    "\n",
    "# === Przewidź kolory ===\n",
    "with torch.no_grad():\n",
    "    output = model(bw_tensor)\n",
    "\n",
    "# === Przeskaluj wynik do oryginalnego rozmiaru ===\n",
    "output_resized = transforms.functional.resize(output.squeeze(0), original_size)\n",
    "\n",
    "# === Zapisz wynik ===\n",
    "save_image(output_resized, output_path)\n",
    "print(f\"Zapisano kolorowy obraz o oryginalnej rozdzielczości: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
