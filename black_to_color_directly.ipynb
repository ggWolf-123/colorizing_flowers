{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f937cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5065d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, color_dir, bw_dir, transform_color=None, transform_bw=None):\n",
    "        self.color_dir = Path(color_dir)\n",
    "        self.bw_dir = Path(bw_dir)\n",
    "        self.transform_color = transform_color\n",
    "        self.transform_bw = transform_bw\n",
    "\n",
    "        color_files = list(self.color_dir.glob(\"*.*\"))\n",
    "        bw_files = list(self.bw_dir.glob(\"*.*\"))\n",
    "        \n",
    "        bw_dict = {}\n",
    "        for f in bw_files:\n",
    "            name = f.stem.lower()\n",
    "            if name.endswith(\"_czb\"):\n",
    "                name = name[:-4]  # usuń '_czb'\n",
    "            bw_dict[name] = f\n",
    "        \n",
    "        self.paired_files = []\n",
    "        \n",
    "        for color_path in color_files:\n",
    "            stem = color_path.stem.lower()\n",
    "            if stem in bw_dict:\n",
    "                self.paired_files.append((color_path, bw_dict[stem]))\n",
    "            else:\n",
    "                print(f\"[!] Brak czb dla: {color_path.stem}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paired_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        color_path, bw_path = self.paired_files[idx]\n",
    "\n",
    "        color_img = Image.open(color_path).convert(\"RGB\")\n",
    "        bw_img = Image.open(bw_path).convert(\"L\")\n",
    "\n",
    "        if self.transform_color:\n",
    "            color_img = self.transform_color(color_img)\n",
    "        if self.transform_bw:\n",
    "            bw_img = self.transform_bw(bw_img)\n",
    "\n",
    "        return bw_img, color_img\n",
    "\n",
    "\n",
    "    \n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "\n",
    "        self.enc1 = conv_block(1, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.enc2 = conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.enc3 = conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.enc4 = conv_block(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = conv_block(512, 1024)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4 = conv_block(1024, 512)\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3 = conv_block(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2 = conv_block(256, 128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1 = conv_block(128, 64)\n",
    "\n",
    "        self.final = nn.Conv2d(64, 3, kernel_size=1)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool1(e1))\n",
    "        e3 = self.enc3(self.pool2(e2))\n",
    "        e4 = self.enc4(self.pool3(e3))\n",
    "\n",
    "        b = self.bottleneck(self.pool4(e4))\n",
    "\n",
    "        d4 = self.dec4(torch.cat([self.up4(b), e4], dim=1))\n",
    "        d3 = self.dec3(torch.cat([self.up3(d4), e3], dim=1))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "\n",
    "        out = self.final(d1)\n",
    "        return self.activation(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f8c61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liczba sparowanych plików w dataset: 21625\n"
     ]
    }
   ],
   "source": [
    "# Transformaty dla czarno-białych i kolorowych obrazów\n",
    "transform_color = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.3, hue=0.05),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,)*3, (0.5,)*3)\n",
    "])\n",
    "\n",
    "transform_bw = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.RandomHorizontalFlip(),  # zsynchronizowany z kolorem\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "# === Ścieżki ===\n",
    "color_dir = \"\"      #folder z kwiatami kolorowymi\n",
    "bw_dir = \"\"         #folder z czarno-białymi\n",
    "\n",
    "# Dataset\n",
    "dataset = ColorizationDataset(color_dir, bw_dir, transform_color, transform_bw)\n",
    "\n",
    "print(f\"Liczba sparowanych plików w dataset: {len(dataset)}\")\n",
    "\n",
    "# DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# liczba sparowanych plików powinna wynosić 21625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f582749f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epo 1/5] Epoch [1/4], Loss: 0.0481\n",
      "[Epo 1/5] Epoch [2/4], Loss: 0.0383\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epo = 5\n",
    "num_epochs = 4\n",
    "\n",
    "# Pełny dataset (utwórz raz)\n",
    "full_dataset = ColorizationDataset(\n",
    "    \n",
    "    # === Ścieżki ===\n",
    "    color_dir=\"\",         #folder z kwiatami kolorowymi  \n",
    "    bw_dir=\"\",            #folder z czarno-białymi  \n",
    "    \n",
    "    transform_color=transform_color,\n",
    "    transform_bw=transform_bw\n",
    ")\n",
    "\n",
    "for epos in range(epo):\n",
    "    # === LOSUJEMY 500 losowych obrazów na aktualny cykl treningowy ===\n",
    "    subset_indices = random.sample(range(len(full_dataset)), min(500, len(full_dataset)))\n",
    "    subset = Subset(full_dataset, subset_indices)\n",
    "    dataloader = DataLoader(subset, batch_size=8, shuffle=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for bw, color in dataloader:\n",
    "            bw, color = bw.to(device), color.to(device)\n",
    "\n",
    "            output = model(bw)\n",
    "            loss = criterion(output, color)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"[Epo {epos+1}/{epo}] Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # === Zapis modelu po każdej pełnej epoce (czyli po num_epochs iteracjach) ===\n",
    "    save_path = f\"C:\\\\zdjęcia na chwile\\\\kwiaty_same_model_i_data\\\\model_czb_to_kol_kwiaty_{epos+1}_{loss.item():.4f}.pth\"  #przykładowa ścieżka \\ sample source\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f\"Zapisano model: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28342263",
   "metadata": {},
   "source": [
    "dodatkowy trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6932e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UNet().to(device)\n",
    "\n",
    "model_path=\"\"    #model który chcesz dotrenować\n",
    "\n",
    "\n",
    "# Załaduj zapisane wcześniej wagi modelu\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "epo = 5\n",
    "num_epochs = 4\n",
    "for i in range(epo):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for bw, color in dataloader:\n",
    "            bw, color = bw.to(device), color.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(bw)\n",
    "            loss = criterion(output, color)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * bw.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "    zapis_path=f\"\"    #gdzie zapisać ten model (najlepiej daj tą samą nazwę ale z jakimś dopiskiem np. \"_tren_{epo+1}_{epoch_loss:.4f}.pth\")\n",
    "    torch.save(model.state_dict(), zapis_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3847b9",
   "metadata": {},
   "source": [
    "WCZYTANIE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61946f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapisano kolorowy obraz o oryginalnej rozdzielczości: C:\\zdjęcia na chwile\\kwiaty_same_model_i_data\\wynik_kolor_fullres.png\n"
     ]
    }
   ],
   "source": [
    "# === Ścieżki ===\n",
    "input_path = \"\"\n",
    "output_path = \"\"\n",
    "model_path =\"\"\n",
    "\n",
    "# === Wczytaj model ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = UNet().to(device)\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# === Wczytaj obraz czarno-biały ===\n",
    "if not os.path.exists(input_path):\n",
    "    print(f\"Plik {input_path} nie istnieje.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "img = Image.open(input_path).convert(\"L\")\n",
    "original_size = img.size  # zapamiętaj oryginalny rozmiar\n",
    "\n",
    "# === Skaluj do 128x128 dla modelu ===\n",
    "transform_bw = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "bw_tensor = transform_bw(img).unsqueeze(0).to(device)\n",
    "\n",
    "# === Przewidź kolory ===\n",
    "with torch.no_grad():\n",
    "    output = model(bw_tensor)\n",
    "\n",
    "# === Przeskaluj wynik do oryginalnego rozmiaru ===\n",
    "output_resized = transforms.functional.resize(output.squeeze(0), original_size)\n",
    "\n",
    "# === Zapisz wynik ===\n",
    "save_image(output_resized, output_path)\n",
    "print(f\"Zapisano kolorowy obraz o oryginalnej rozdzielczości: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
